# [Introduction to Statistical Learning](https://www-bcf.usc.edu/~gareth/ISL/)
**Authors:** Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani


The book was taught at Stanford University in summer of 2016 by the authors. The MOOC can be found [here](https://lagunita.stanford.edu/courses/HumanitiesSciences/StatLearning/Winter2016/about).

This repo contains my *attempted* solutions to exercises from the book. Code is in **Python.** A [series](https://alisiina.github.io/2019/01/28/statistical-learning-series.html) of posts on my [blog](https://alisiina.github.io) summarize concepts from the book.


#### Other resources
* [Solutions in R](https://blog.princehonest.com/stat-learning/)
* [Solutions in Python](https://botlnec.github.io/islp/)
* [`Labs` in R and Python](https://www.science.smith.edu/~jcrouser/SDS293/labs/), Smith College

> The exercises in this book rely heavily on tests of *statistical significance.* Read the article ["Scientists rise up against statistical significance"](https://www.nature.com/articles/d41586-019-00857-9) on why this is a problematic approach. 

#### Datasets
Provided on the [website](https://www-bcf.usc.edu/~gareth/ISL/data.html) and on [CRAN](https://cran.r-project.org/web/packages/ISLR/index.html) as an R package under [GNU General Public License V2](https://cran.r-project.org/web/licenses/GPL-2).


#### Figures
Figures in the posts are created by myself, the code for which can be found as [Gists](https://gist.github.com/alisiina). All figures are free of any copyrights.


## Notebooks of `Applied` exercises
### Chapter 2
[![jupyter](https://img.shields.io/badge/launch-nbviewer-orange.svg?logo=jupyter)](https://nbviewer.jupyter.org/github/alisiina/stat-learning/blob/master/ch2_applied.ipynb)

Simple visual analyses.

### Chapter 3
[![jupyter](https://img.shields.io/badge/launch-nbviewer-orange.svg?logo=jupyter)](https://nbviewer.jupyter.org/github/alisiina/stat-learning/blob/master/ch3_applied.ipynb)

Linear regression. Ample use of [Statsmodels](https://www.statsmodels.org/stable/index.html) package.

### Chapter 4
[![jupyter](https://img.shields.io/badge/launch-nbviewer-orange.svg?logo=jupyter)](https://nbviewer.jupyter.org/github/alisiina/stat-learning/blob/master/ch4_applied.ipynb)

Classification with logistic regression, discriminant analysis, and KNN.

### Chapter 5
[![jupyter](https://img.shields.io/badge/launch-nbviewer-orange.svg?logo=jupyter)](https://nbviewer.jupyter.org/github/alisiina/stat-learning/blob/master/ch5_applied.ipynb)

Leave-One-Out (LOO), K-fold cross validation, and bootstrap.

### Chapter 6
[![jupyter](https://img.shields.io/badge/launch-nbviewer-orange.svg?logo=jupyter)](https://nbviewer.jupyter.org/github/alisiina/stat-learning/blob/master/ch6_applied.ipynb)

Best subset selection: Best subset, forward stepwise, and backward stepwise. Regularization and shrinkage: Lasso, Ridge, PCR, and PLS (with CV here and there).

### Chapter 7
[![jupyter](https://img.shields.io/badge/launch-nbviewer-orange.svg?logo=jupyter)](https://nbviewer.jupyter.org/github/alisiina/stat-learning/blob/master/ch7_applied.ipynb)

Mainly nonlinear algorithms like polynomial regression, ANOVA hypothesis testing, step functions, splines, and GAMS. *This chapter has quite a few questions skipped.*

### Chapter 8
[![jupyter](https://img.shields.io/badge/launch-nbviewer-orange.svg?logo=jupyter)](https://nbviewer.jupyter.org/github/alisiina/stat-learning/blob/master/ch8_applied.ipynb)

Random forests, decisions trees, bagging, and boosting.


### Chapter 9
[![jupyter](https://img.shields.io/badge/launch-nbviewer-orange.svg?logo=jupyter)](https://nbviewer.jupyter.org/github/alisiina/stat-learning/blob/master/ch9_applied.ipynb)

Support Vector Machines (SVM).

### Chapter 10
[![jupyter](https://img.shields.io/badge/launch-nbviewer-orange.svg?logo=jupyter)](https://nbviewer.jupyter.org/github/alisiina/stat-learning/blob/master/ch10_applied.ipynb)

Only hierarchical clustering. Skipped PCA, K-Means exercises.
